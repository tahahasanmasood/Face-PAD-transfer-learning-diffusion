{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment: pt_fpad  \n",
    "Python: 3.10.4     \n",
    "Pytorch: 2.1.1+cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms, models\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_label_real = 0\n",
    "class_label_attack = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replay Mobile Dataset:\n",
    "\n",
    "Training -> 312 (Real 120, Attack 192) ->  312/1030 * 100 = 30.29%  \n",
    "Validation -> 416 (Real 160, Attack 256) -> 416/1030 * 100 = 40.38%  \n",
    "Testing -> 302 (Real 110, Attack 192) -> 302/1030 * 100 = 29.32%  \n",
    "Total = 1030  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Mobile\n",
    "data_path_train_real_RM = '/home/taha/FASdatasets/Sohail/Replay-Mobile/train/real'\n",
    "data_path_train_attack_RM = '/home/taha/FASdatasets/Sohail/Replay-Mobile/train/attack'\n",
    "\n",
    "data_path_devel_real_RM = '/home/taha/FASdatasets/Sohail/Replay-Mobile/devel/real'\n",
    "data_path_devel_attack_RM = '/home/taha/FASdatasets/Sohail/Replay-Mobile/devel/attack'\n",
    "\n",
    "# Replay Attack (Real)\n",
    "\n",
    "data_path_train_real_RA = '/home/taha/FASdatasets/Sohail/Replay_Attack/train/real'\n",
    "data_path_devel_real_RA = '/home/taha/FASdatasets/Sohail/Replay_Attack/devel/real'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_samples(path, class_label, transform): #Select N frames returned from read_all_frames and assign labels to all samples of same class\n",
    "        frames = read_all_frames(path)\n",
    "        total_frames = list(range(0, frames.shape[0], 1))\n",
    "        selected_samples = random.sample(total_frames, 1)\n",
    "        samples =[]\n",
    "        # Assign the same class label to all samples\n",
    "        label = class_label\n",
    "        samples =(transform(frames[selected_samples].squeeze()), label)     \n",
    "        return samples\n",
    "\n",
    "def read_all_frames(video_path): # reads all frames from a particular video and converts them to PyTorch tensors.\n",
    "    frame_list = []\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    success = True\n",
    "    while success:\n",
    "        success, frame = video.read()\n",
    "        if success:\n",
    "            frame = cv2.resize(frame, (224, 224), interpolation=cv2.INTER_AREA) #framesize kept 40, 30 as mentioned in paper but 224, 224 is also fine \n",
    "            frame_list.append(frame)\n",
    "    frame_list = np.array(frame_list)\n",
    "    return frame_list\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, data_path, class_label):\n",
    "        self.data_path = data_path #path for directory containing video files\n",
    "        self.video_files = [file for file in os.listdir(data_path) if file.endswith('.mov') or file.endswith('.mp4')] #list of video files in the specified directory #.mov for RA and RM, .mp4 for RY\n",
    "        self.class_label = class_label #manually assign class_label for your desired class while loading\n",
    "        self.data_length = len(self.video_files) \n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def __len__(self): # returns the total number of samples in the dataset\n",
    "        return self.data_length\n",
    "\n",
    "    def __getitem__(self, idx): # loads and returns a sample from the dataset at the given index\n",
    "        file = self.video_files[idx]\n",
    "        path = os.path.join(self.data_path, file)\n",
    "        frames= load_samples(path, self.class_label, self.transform)\n",
    "\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Mobile Dataset\n",
    "train_dataset_real_RM = VideoDataset(data_path_train_real_RM, class_label_real)\n",
    "train_dataset_attack_RM = VideoDataset(data_path_train_attack_RM, class_label_attack)\n",
    "\n",
    "val_dataset_real_RM = VideoDataset(data_path_devel_real_RM, class_label_real)\n",
    "val_dataset_attack_RM = VideoDataset(data_path_devel_attack_RM, class_label_attack)\n",
    "\n",
    "# Replay Attack Dataset (Real)\n",
    "train_dataset_real_RA = VideoDataset(data_path_train_real_RA, class_label_real)\n",
    "val_dataset_real_RA = VideoDataset(data_path_devel_real_RA, class_label_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Mobile DataLoader\n",
    "train_loader_real_RM = DataLoader(train_dataset_real_RM, batch_size=1, shuffle=True)\n",
    "train_loader_attack_RM = DataLoader(train_dataset_attack_RM, batch_size=1, shuffle=True)\n",
    "\n",
    "val_loader_real_RM = DataLoader(val_dataset_real_RM, batch_size=1, shuffle=False)\n",
    "val_loader_attack_RM = DataLoader(val_dataset_attack_RM, batch_size=1, shuffle=False)\n",
    "\n",
    "# Replay Attack DataLoader\n",
    "\n",
    "train_loader_real_RA = DataLoader(train_dataset_real_RA, batch_size=1, shuffle=True)\n",
    "val_loader_real_RA = DataLoader(val_dataset_real_RA, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_train_dataset_RM_RAR = ConcatDataset([train_dataset_real_RM, train_dataset_attack_RM, train_dataset_real_RA])\n",
    "concatenated_val_dataset_RM_RAR = ConcatDataset([val_dataset_real_RM, val_dataset_attack_RM, val_dataset_real_RA])\n",
    "\n",
    "concatenated_train_loader = DataLoader(concatenated_train_dataset_RM_RAR, batch_size=64, shuffle=True, pin_memory=True, num_workers=8)\n",
    "concatenated_val_loader = DataLoader(concatenated_val_dataset_RM_RAR, batch_size=64, shuffle=False, pin_memory=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dataset sizes\n",
    "print(f\"Training set size: {len(concatenated_train_dataset_RM_RAR)}\")\n",
    "print(f\"Validation set size: {len(concatenated_val_dataset_RM_RAR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet18\n",
    "# model = models.resnet18(pretrained=True)\n",
    "# num_ftrs = model.fc.in_features\n",
    "# model.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "# Load pre-trained MobileNetV2\n",
    "model = models.mobilenet_v2(pretrained=True)\n",
    "model.classifier[1] = nn.Linear(in_features=1280, out_features=2) #default in_features =1280, out_features = 1000\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Set up early stopping parameters\n",
    "patience = 5  # Number of epochs with no improvement after which training will be stopped\n",
    "best_loss = float('inf') #set to positive infinity to ensure that the first validation loss encountered will always be considered an improvement\n",
    "counter = 0  # Counter to keep track of consecutive epochs with no improvement\n",
    "\n",
    "#Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    train_correct_predictions = 0\n",
    "    total_train_samples = 0\n",
    "\n",
    "    for train_images, train_labels in concatenated_train_loader:\n",
    "        train_images, train_labels = train_images.to(device), train_labels.to(device)\n",
    "        # Clear the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Forward Pass\n",
    "        train_outputs = model(train_images)\n",
    "        # Find the Loss\n",
    "        train_loss = criterion(train_outputs, train_labels)\n",
    "        # Calculate gradients\n",
    "        train_loss.backward()\n",
    "        # Update Weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # accumulate the training loss\n",
    "        running_loss += train_loss.item()\n",
    "\n",
    "        # calculate training accuracy\n",
    "        _, train_predicted = torch.max(train_outputs, 1) # _ contain max value, train_predicted contain the indices where maximum value occured\n",
    "        train_correct_predictions += (train_predicted == train_labels).sum().item() \n",
    "        total_train_samples += train_labels.size(0)\n",
    "            \n",
    "    train_total_loss = running_loss / len(concatenated_train_loader)\n",
    "    train_accuracy = train_correct_predictions / total_train_samples * 100\n",
    "    train_losses.append(train_total_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "\n",
    "    val_running_loss = 0.0\n",
    "    val_correct_prediction = 0\n",
    "    total_val_samples = 0\n",
    "\n",
    "    #Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_labels in concatenated_val_loader:\n",
    "            val_images, val_labels = val_images.to(device), val_labels.to(device)\n",
    "            val_op = model(val_images)\n",
    "            val_loss = criterion(val_op, val_labels)\n",
    "            val_running_loss += val_loss.item()\n",
    "\n",
    "            _, val_predicted = torch.max(val_op, 1)\n",
    "            val_correct_prediction += (val_predicted == val_labels).sum().item()\n",
    "            total_val_samples += val_labels.size(0)\n",
    "        \n",
    "        val_total_loss = val_running_loss / len(concatenated_val_loader)\n",
    "        val_accuracy = val_correct_prediction / total_val_samples * 100\n",
    "        val_losses.append(val_total_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "    # Check if validation loss has improved\n",
    "    if val_total_loss < best_loss:\n",
    "        best_loss = val_total_loss\n",
    "        counter = 0\n",
    "        # Save the model if needed\n",
    "        torch.save(model.state_dict(), 'RM_RAR_best_model.pth')\n",
    "\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "        # Check if training should be stopped\n",
    "        if counter >= patience:\n",
    "            print(f'Early stopping at epoch {epoch}')\n",
    "            break\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Training Loss: {train_total_loss:.4f}, Training Accuracy: {train_accuracy:.2f}%, Validation Loss: {val_total_loss: .4f}, Best Loss: {best_loss: .4f}, Validation Accuracy: {val_accuracy:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt_fpad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
